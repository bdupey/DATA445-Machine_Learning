{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions\n",
    "\n",
    "1. If a decision tree is under-fitting the training dataset, is it a good idea to try scaling\n",
    "the input features?\n",
    "\n",
    "No. Underfitting is a symptom of a problem with the models complexity. Where scaling the data deals with the magnitude / range of feature values. So scaling the data wouldn't really provide assistance with the models complexity.\n",
    "\n",
    "2. If a decision tree is over-fitting the training dataset, is it a good idea to try decreasing\n",
    "max depth?\n",
    "\n",
    "Yes. Reducing the max_depth decreases how deep the tree can grow, which affects the complexity of the model. If a model is overfitting, then reducing the max_depth can decrease the complexity. Which can assist in problems with over-fitting.\n",
    "\n",
    "3. Why would you use a random forest instead of a decision tree?\n",
    "\n",
    "(f) (b) and (c)\n",
    "\n",
    "4. Which of the following is/are TRUE about bagging trees?\n",
    "\n",
    "(d) (a) and (c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 5:\n",
      "The RMSE of the Linear Regression Model is 1660.03\n",
      "The RMSE of the Bagged Linear Regression model is 1654.50\n",
      "The RMSE of the Ridge model is 1662.88\n",
      "The RMSE of the Bagged Ridge model is 1657.15\n",
      "The RMSE of the Lasso model is 1660.03\n",
      "The RMSE of the Bagged LASSO model is 1654.50\n",
      "I would use the bagged linear regression model with the lowest RMSE of 1654.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, recall_score\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestClassifier, ExtraTreesClassifier\n",
    "# Exercise 5\n",
    "print(\"Exercise 5:\")\n",
    "best_rmse = float('inf')\n",
    "best_model = None\n",
    "# a\n",
    "college = pd.read_csv('College.csv')\n",
    "# b\n",
    "college['Private'] = college['Private'].map({'Yes': 1, 'No': 0})\n",
    "# c\n",
    "X = college[['Private', 'F.Undergrad', 'P.Undergrad', 'Outstate', 'Room.Board', 'Books', 'Personal', 'S.F.Ratio', 'Grad.Rate']]\n",
    "y = college['Apps']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# d\n",
    "linReg_md = make_pipeline(StandardScaler(), LinearRegression())\n",
    "linReg_md.fit(X_train, y_train)\n",
    "linReg_pred = linReg_md.predict(X_test)\n",
    "linReg_mse = mean_squared_error(y_test, linReg_pred)\n",
    "linReg_rmse=np.sqrt(linReg_mse)\n",
    "print(f\"The RMSE of the Linear Regression Model is {linReg_rmse:.2f}\")\n",
    "best_rmse=linReg_rmse\n",
    "best_model=\"Linear Regression\"\n",
    "# e\n",
    "bag_linReg_md=BaggingRegressor(estimator=linReg_md, n_estimators=50, random_state=42)\n",
    "bag_linReg_md.fit(X_train, y_train)\n",
    "bag_linReg_pred=bag_linReg_md.predict(X_test)\n",
    "bag_lin_Reg_mse=mean_squared_error(y_test, bag_linReg_pred)\n",
    "bag_lin_Reg_rmse=np.sqrt(bag_lin_Reg_mse)\n",
    "print(f\"The RMSE of the Bagged Linear Regression model is {bag_lin_Reg_rmse:.2f}\")\n",
    "if bag_lin_Reg_rmse < best_rmse:\n",
    "    best_rmse = bag_lin_Reg_rmse\n",
    "    best_model = \"bagged linear regression\"\n",
    "# f\n",
    "ridge_lambda=[]\n",
    "lambdas_to_consider=np.linspace(0.001, 100, num = 100)\n",
    "ridge_cv = RidgeCV(alphas=lambdas_to_consider, cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "optimal_lambda = ridge_cv.alpha_\n",
    "ridge_md = make_pipeline(StandardScaler(), Ridge(alpha=optimal_lambda))\n",
    "ridge_md.fit(X_train, y_train)\n",
    "ridge_pred = ridge_md.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "ridge_rmse = np.sqrt(ridge_mse)\n",
    "print(f'The RMSE of the Ridge model is {ridge_rmse:.2f}')\n",
    "if ridge_rmse < best_rmse:\n",
    "    best_rmse = ridge_rmse\n",
    "    best_model = \"Ridge regression\"\n",
    "# g\n",
    "bag_ridge_md=BaggingRegressor(estimator=ridge_md, n_estimators=50, random_state=42)\n",
    "bag_ridge_md.fit(X_train, y_train)\n",
    "bag_ridge_pred=bag_ridge_md.predict(X_test)\n",
    "bag_ridge_mse=mean_squared_error(y_test, bag_ridge_pred)\n",
    "bag_ridge_rmse=np.sqrt(bag_ridge_mse)\n",
    "print(f\"The RMSE of the Bagged Ridge model is {bag_ridge_rmse:.2f}\")\n",
    "if bag_ridge_rmse < best_rmse:\n",
    "    best_rmse = bag_ridge_rmse\n",
    "    best_model = \"Bagged Ridge\"\n",
    "# h\n",
    "lasso_cv = LassoCV(alphas=np.linspace(0.001, 100, num=100), cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "optimal_lambda = lasso_cv.alpha_\n",
    "lasso_md = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Lasso(alpha=optimal_lambda))\n",
    "])\n",
    "lasso_md.fit(X_train, y_train)\n",
    "lasso_md_pred=lasso_md.predict(X_test)\n",
    "lasso_md_mse=mean_squared_error(y_test, lasso_md_pred)\n",
    "lasso_md_rmse=np.sqrt(lasso_md_mse)\n",
    "print(f\"The RMSE of the Lasso model is {lasso_md_rmse:.2f}\")\n",
    "if lasso_md_rmse < best_rmse:\n",
    "    best_rmse = lasso_md_rmse\n",
    "    best_model = \"Lasso\"\n",
    "# i\n",
    "bag_lasso_md = BaggingRegressor(estimator=lasso_md, n_estimators=50, random_state=42)\n",
    "bag_lasso_md.fit(X_train, y_train)\n",
    "bag_lasso_pred = bag_lasso_md.predict(X_test)\n",
    "bag_lasso_mse = mean_squared_error(y_test, bag_lasso_pred)\n",
    "bag_lasso_rmse = np.sqrt(bag_lasso_mse)\n",
    "print(f\"The RMSE of the Bagged LASSO model is {bag_lasso_rmse:.2f}\")\n",
    "if bag_lasso_rmse < best_rmse:\n",
    "    best_rmse = bag_lasso_rmse\n",
    "    best_model = \"Bagged LASSO\"\n",
    "# j\n",
    "print(f\"I would use the {best_model} model with the lowest RMSE of {best_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:24<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictor variables based on average importance:\n",
      "sysBP\n",
      "BMI\n",
      "age\n",
      "totChol\n",
      "glucose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6\n",
    "print(\"Exercise 6:\")\n",
    "# a\n",
    "heart = pd.read_csv('framingham.csv')\n",
    "# b\n",
    "heart.dropna(inplace=True)\n",
    "# ci\n",
    "X = heart.drop('TenYearCHD', axis=1)\n",
    "y = heart['TenYearCHD']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "# cii\n",
    "rf_classifier = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "# ciii\n",
    "feature_importance = rf_classifier.feature_importances_\n",
    "# Part c repetition loop\n",
    "feat_imps = []\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Extract feature importance and store it\n",
    "    feat_imp = rf_classifier.feature_importances_\n",
    "    feat_imps.append(feat_imp)\n",
    "\n",
    "avg_imps = np.mean(feat_imps, axis=0)\n",
    "sorted_indices = np.argsort(avg_imps)[::-1]\n",
    "top_5_indices = sorted_indices[:5]\n",
    "top_5_features = X.columns[top_5_indices]\n",
    "\n",
    "print(\"Top 5 predictor variables based on average importance:\")\n",
    "for feature in top_5_features:\n",
    "    print(feature)\n",
    "# di\n",
    "X_top5 = heart[top_5_features]\n",
    "y = heart['TenYearCHD']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top5, y, test_size=0.2, stratify=y)\n",
    "# dii\n",
    "rf_classifier_1 = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=42)\n",
    "rf_classifier_1.fit(X_train, y_train)\n",
    "y_pred_proba_1 = rf_classifier_1.predict_proba(X_test)[:, 1]\n",
    "pred_label_1=np.where(y_pred_proba_1<0.1,0,1)\n",
    "recall_1 = recall_score(y_test, pred_label_1)\n",
    "print(\"Recall of RF 1:\", recall_1)\n",
    "# diii\n",
    "rf_classifier_2 = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\n",
    "rf_classifier_2.fit(X_train, y_train)\n",
    "y_pred_proba_2 = rf_classifier_2.predict_proba(X_test)[:, 1]\n",
    "pred_label_2 = np.where(y_pred_proba_2 < 0.1, 0, 1)\n",
    "recall_2 = recall_score(y_test, pred_label_2)\n",
    "print(\"Recall of RF 2:\", recall_2)\n",
    "# div\n",
    "rf_classifier_4 = RandomForestClassifier(n_estimators=500, max_depth=7, random_state=42)\n",
    "rf_classifier_4.fit(X_train, y_train)\n",
    "y_pred_proba_4 = rf_classifier_4.predict_proba(X_test)[:, 1]\n",
    "pred_label_4 = np.where(y_pred_proba_4 < 0.1, 0, 1)\n",
    "recall_4 = recall_score(y_test, pred_label_4)\n",
    "print(\"Recall of RF 4:\", recall_4)\n",
    "# dv\n",
    "et_classifier_1 = ExtraTreesClassifier(n_estimators=500, max_depth=3, random_state=42)\n",
    "et_classifier_1.fit(X_train, y_train)\n",
    "y_pred_proba_et_1 = et_classifier_1.predict_proba(X_test)[:, 1]\n",
    "pred_label_et_1 = np.where(y_pred_proba_et_1 < 0.1, 0, 1)\n",
    "recall_et_1 = recall_score(y_test, pred_label_et_1)\n",
    "print(\"Recall of RF 5 (ExtraTrees):\", recall_et_1)\n",
    "# dvi\n",
    "et_classifier_2 = ExtraTreesClassifier(n_estimators=500, max_depth=5, random_state=42)\n",
    "et_classifier_2.fit(X_train, y_train)\n",
    "y_pred_proba_et_2 = et_classifier_2.predict_proba(X_test)[:, 1]\n",
    "pred_label_et_2 = np.where(y_pred_proba_et_2 < 0.1, 0, 1)\n",
    "recall_et_2 = recall_score(y_test, pred_label_et_2)\n",
    "print(\"Recall of ET 2 (ExtraTrees):\", recall_et_2)\n",
    "# dvii\n",
    "et_classifier_3 = ExtraTreesClassifier(n_estimators=500, max_depth=7, random_state=42)\n",
    "et_classifier_3.fit(X_train, y_train)\n",
    "y_pred_proba_et_3 = et_classifier_3.predict_proba(X_test)[:, 1]\n",
    "pred_label_et_3 = np where(y_pred_proba_et_3 < 0.1, 0, 1)\n",
    "recall_et_3 = recall_score(y_test, pred_label_et_3)\n",
    "print(\"Recall of ET 3 (ExtraTrees):\", recall_et_3)\n",
    "# Part d repetition loop\n",
    "avg_recall_rf1 = 0\n",
    "avg_recall_rf2 = 0\n",
    "avg_recall_rf4 = 0\n",
    "avg_recall_et1 = 0\n",
    "avg_recall_et2 = 0\n",
    "avg_recall_et3 = 0\n",
    "for _ in range(100):\n",
    "    X_top5 = heart[top_5_features]\n",
    "    y = heart['TenYearCHD']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_top5, y, test_size=0.2, stratify=y)\n",
    "    # rf_1\n",
    "    rf_classifier_1 = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=42)\n",
    "    rf_classifier_1.fit(X_train, y_train)\n",
    "    y_pred_proba_1 = rf_classifier_1.predict_proba(X_test)[:, 1]\n",
    "    pred_label_1 = np.where(y_pred_proba_1 < 0.1, 0, 1)\n",
    "    recall_1 = recall_score(y_test, pred_label_1)\n",
    "    avg_recall_rf1 += recall_1\n",
    "    # rf_2\n",
    "    rf_classifier_2 = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\n",
    "    rf_classifier_2.fit(X_train, y_train)\n",
    "    y_pred_proba_2 = rf_classifier_2.predict_proba(X_test)[:, 1]\n",
    "    pred_label_2 = np.where(y_pred_proba_2 < 0.1, 0, 1)\n",
    "    recall_2 = recall_score(y_test, pred_label_2)\n",
    "    avg_recall_rf2 += recall_2\n",
    "    # rf_3\n",
    "    rf_classifier_4 = RandomForestClassifier(n_estimators=500, max_depth=7, random_state=42)\n",
    "    rf_classifier_4.fit(X_train, y_train)\n",
    "    y_pred_proba_4 = rf_classifier_4.predict_proba(X_test)[:, 1]\n",
    "    pred_label_4 = np.where(y_pred_proba_4 < 0.1, 0, 1)\n",
    "    recall_4 = recall_score(y_test, pred_label_4)\n",
    "    avg_recall_rf4 += recall_4\n",
    "    # et_1\n",
    "    et_classifier_1 = ExtraTreesClassifier(n_estimators=500, max_depth=3, random_state=42)\n",
    "    et_classifier_1.fit(X_train, y_train)\n",
    "    y_pred_proba_et_1 = et_classifier_1.predict_proba(X_test)[:, 1]\n",
    "    pred_label_et_1 = np.where(y_pred_proba_et_1 < 0.1, 0, 1)\n",
    "    recall_et_1 = recall_score(y_test, pred_label_et_1)\n",
    "    avg_recall_et1 += recall_et_1\n",
    "    # et_2\n",
    "    et_classifier_2 = ExtraTreesClassifier(n_estimators=500, max_depth=5, random_state=42)\n",
    "    et_classifier_2.fit(X_train, y_train)\n",
    "    y_pred_proba_et_2 = et_classifier_2.predict_proba(X_test)[:, 1]\n",
    "    pred_label_et_2 = np.where(y_pred_proba_et_2 < 0.1, 0, 1)\n",
    "    recall_et_2 = recall_score(y_test, pred_label_et_2)\n",
    "    avg_recall_et2 += recall_et_2\n",
    "    # et_3\n",
    "    et_classifier_3 = ExtraTreesClassifier(n_estimators=500, max_depth=7, random_state=42)\n",
    "    et_classifier_3.fit(X_train, y_train)\n",
    "    y_pred_proba_et_3 = et_classifier_3.predict_proba(X_test)[:, 1]\n",
    "    pred_label_et_3 = np.where(y_pred_proba_et_3 < 0.1, 0, 1)\n",
    "    recall_et_3 = recall_score(y_test, pred_label_et_3)\n",
    "    avg_recall_et3 += recall_et_3\n",
    "# Calculate the average recall across 100 iterations\n",
    "avg_recall_rf1 /= 100\n",
    "avg_recall_rf2 /= 100\n",
    "avg_recall_rf4 /= 100\n",
    "avg_recall_et1 /= 100\n",
    "avg_recall_et2 /= 100\n",
    "avg_recall_et3 /= 100\n",
    "print(\"Average Recall values for 100 loops:\")\n",
    "print(\"Average Recall of RF 1:\", avg_recall_rf1)\n",
    "print(\"Average Recall of RF 2:\", avg_recall_rf2)\n",
    "print(\"Average Recall of RF 4:\", avg_recall_rf4)\n",
    "print(\"Average Recall of ET 1 (ExtraTrees):\", avg_recall_et1)\n",
    "print(\"Average Recall of ET 2 (ExtraTrees):\", avg_recall_et2)\n",
    "print(\"Average Recall of ET 3 (ExtraTrees):\", avg_recall_et3)\n",
    "best_model = None\n",
    "highest_avg_recall = max(\n",
    "    avg_recall_rf1, avg_recall_rf2, avg_recall_rf4, avg_recall_et1, avg_recall_et2, avg_recall_et3\n",
    ")\n",
    "if highest_avg_recall == avg_recall_rf1:\n",
    "    best_model = \"Random Forest 1\"\n",
    "elif highest_avg_recall == avg_recall_rf2:\n",
    "    best_model = \"Random Forest 2\"\n",
    "elif highest_avg_recall == avg_recall_rf4:\n",
    "    best_model = \"Random Forest 4\"\n",
    "elif highest_avg_recall == avg_recall_et1:\n",
    "    best_model = \"Extra Trees 1\"\n",
    "elif highest_avg_recall == avg_recall_et2:\n",
    "    best_model = \"Extra Trees 2\"\n",
    "elif highest_avg_recall == avg_recall_et3:\n",
    "    best_model = \"Extra Trees 3\"\n",
    "print(\"The model to predict TenYearCHD with the highest average recall with 100 iterations is:\", best_model)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
